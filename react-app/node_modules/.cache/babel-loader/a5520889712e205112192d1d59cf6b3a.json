{"ast":null,"code":"// +----------------------------------------------------------------------+\n// | murmurHash3js.js v3.0.1 // https://github.com/pid/murmurHash3js\n// | A javascript implementation of MurmurHash3's x86 hashing algorithms. |\n// |----------------------------------------------------------------------|\n// | Copyright (c) 2012-2015 Karan Lyons                                       |\n// | https://github.com/karanlyons/murmurHash3.js/blob/c1778f75792abef7bdd74bc85d2d4e1a3d25cfe9/murmurHash3.js |\n// | Freely distributable under the MIT license.                          |\n// +----------------------------------------------------------------------+\n// PRIVATE FUNCTIONS\n// -----------------\nfunction _x86Multiply(m, n) {\n  //\n  // Given two 32bit ints, returns the two multiplied together as a\n  // 32bit int.\n  //\n  return (m & 0xffff) * n + (((m >>> 16) * n & 0xffff) << 16);\n}\n\nfunction _x86Rotl(m, n) {\n  //\n  // Given a 32bit int and an int representing a number of bit positions,\n  // returns the 32bit int rotated left by that number of positions.\n  //\n  return m << n | m >>> 32 - n;\n}\n\nfunction _x86Fmix(h) {\n  //\n  // Given a block, returns murmurHash3's final x86 mix of that block.\n  //\n  h ^= h >>> 16;\n  h = _x86Multiply(h, 0x85ebca6b);\n  h ^= h >>> 13;\n  h = _x86Multiply(h, 0xc2b2ae35);\n  h ^= h >>> 16;\n  return h;\n}\n\nfunction _x64Add(m, n) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // added together as a 64bit int (as an array of two 32bit ints).\n  //\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n  o[3] += m[3] + n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n  o[2] += m[2] + n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n  o[1] += m[1] + n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[0] += m[0] + n[0];\n  o[0] &= 0xffff;\n  return [o[0] << 16 | o[1], o[2] << 16 | o[3]];\n}\n\nfunction _x64Multiply(m, n) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // multiplied together as a 64bit int (as an array of two 32bit ints).\n  //\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n  o[3] += m[3] * n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n  o[2] += m[2] * n[3];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n  o[2] += m[3] * n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n  o[1] += m[1] * n[3];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[1] += m[2] * n[2];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[1] += m[3] * n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n  o[0] += m[0] * n[3] + m[1] * n[2] + m[2] * n[1] + m[3] * n[0];\n  o[0] &= 0xffff;\n  return [o[0] << 16 | o[1], o[2] << 16 | o[3]];\n}\n\nfunction _x64Rotl(m, n) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) rotated left by that number of positions.\n  //\n  n %= 64;\n\n  if (n === 32) {\n    return [m[1], m[0]];\n  } else if (n < 32) {\n    return [m[0] << n | m[1] >>> 32 - n, m[1] << n | m[0] >>> 32 - n];\n  } else {\n    n -= 32;\n    return [m[1] << n | m[0] >>> 32 - n, m[0] << n | m[1] >>> 32 - n];\n  }\n}\n\nfunction _x64LeftShift(m, n) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) shifted left by that number of positions.\n  //\n  n %= 64;\n\n  if (n === 0) {\n    return m;\n  } else if (n < 32) {\n    return [m[0] << n | m[1] >>> 32 - n, m[1] << n];\n  } else {\n    return [m[1] << n - 32, 0];\n  }\n}\n\nfunction _x64Xor(m, n) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // xored together as a 64bit int (as an array of two 32bit ints).\n  //\n  return [m[0] ^ n[0], m[1] ^ n[1]];\n}\n\nfunction _x64Fmix(h) {\n  //\n  // Given a block, returns murmurHash3's final x64 mix of that block.\n  // (`[0, h[0] >>> 1]` is a 33 bit unsigned right shift. This is the\n  // only place where we need to right shift 64bit ints.)\n  //\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xff51afd7, 0xed558ccd]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xc4ceb9fe, 0x1a85ec53]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  return h;\n} // PUBLIC FUNCTIONS\n// ----------------\n\n\nfunction x86Hash32(bytes, seed) {\n  //\n  // Given a string and an optional seed as an int, returns a 32 bit hash\n  // using the x86 flavor of MurmurHash3, as an unsigned int.\n  //\n  seed = seed || 0;\n  const remainder = bytes.length % 4;\n  const blocks = bytes.length - remainder;\n  let h1 = seed;\n  let k1 = 0;\n  const c1 = 0xcc9e2d51;\n  const c2 = 0x1b873593;\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 4) {\n    k1 = bytes[i] | bytes[i + 1] << 8 | bytes[i + 2] << 16 | bytes[i + 3] << 24;\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n    h1 ^= k1;\n    h1 = _x86Rotl(h1, 13);\n    h1 = _x86Multiply(h1, 5) + 0xe6546b64;\n    j = i + 4;\n  }\n\n  k1 = 0;\n\n  switch (remainder) {\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n\n  h1 ^= bytes.length;\n  h1 = _x86Fmix(h1);\n  return h1 >>> 0;\n}\n\nfunction x86Hash128(bytes, seed) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x86 flavor of MurmurHash3, as an unsigned hex.\n  //\n  seed = seed || 0;\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n  let h1 = seed;\n  let h2 = seed;\n  let h3 = seed;\n  let h4 = seed;\n  let k1 = 0;\n  let k2 = 0;\n  let k3 = 0;\n  let k4 = 0;\n  const c1 = 0x239b961b;\n  const c2 = 0xab0e9789;\n  const c3 = 0x38b34ae5;\n  const c4 = 0xa1e38b93;\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = bytes[i] | bytes[i + 1] << 8 | bytes[i + 2] << 16 | bytes[i + 3] << 24;\n    k2 = bytes[i + 4] | bytes[i + 5] << 8 | bytes[i + 6] << 16 | bytes[i + 7] << 24;\n    k3 = bytes[i + 8] | bytes[i + 9] << 8 | bytes[i + 10] << 16 | bytes[i + 11] << 24;\n    k4 = bytes[i + 12] | bytes[i + 13] << 8 | bytes[i + 14] << 16 | bytes[i + 15] << 24;\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n    h1 ^= k1;\n    h1 = _x86Rotl(h1, 19);\n    h1 += h2;\n    h1 = _x86Multiply(h1, 5) + 0x561ccd1b;\n    k2 = _x86Multiply(k2, c2);\n    k2 = _x86Rotl(k2, 16);\n    k2 = _x86Multiply(k2, c3);\n    h2 ^= k2;\n    h2 = _x86Rotl(h2, 17);\n    h2 += h3;\n    h2 = _x86Multiply(h2, 5) + 0x0bcaa747;\n    k3 = _x86Multiply(k3, c3);\n    k3 = _x86Rotl(k3, 17);\n    k3 = _x86Multiply(k3, c4);\n    h3 ^= k3;\n    h3 = _x86Rotl(h3, 15);\n    h3 += h4;\n    h3 = _x86Multiply(h3, 5) + 0x96cd1c35;\n    k4 = _x86Multiply(k4, c4);\n    k4 = _x86Rotl(k4, 18);\n    k4 = _x86Multiply(k4, c1);\n    h4 ^= k4;\n    h4 = _x86Rotl(h4, 13);\n    h4 += h1;\n    h4 = _x86Multiply(h4, 5) + 0x32ac3b17;\n    j = i + 16;\n  }\n\n  k1 = 0;\n  k2 = 0;\n  k3 = 0;\n  k4 = 0;\n\n  switch (remainder) {\n    case 15:\n      k4 ^= bytes[j + 14] << 16;\n\n    case 14:\n      k4 ^= bytes[j + 13] << 8;\n\n    case 13:\n      k4 ^= bytes[j + 12];\n      k4 = _x86Multiply(k4, c4);\n      k4 = _x86Rotl(k4, 18);\n      k4 = _x86Multiply(k4, c1);\n      h4 ^= k4;\n\n    case 12:\n      k3 ^= bytes[j + 11] << 24;\n\n    case 11:\n      k3 ^= bytes[j + 10] << 16;\n\n    case 10:\n      k3 ^= bytes[j + 9] << 8;\n\n    case 9:\n      k3 ^= bytes[j + 8];\n      k3 = _x86Multiply(k3, c3);\n      k3 = _x86Rotl(k3, 17);\n      k3 = _x86Multiply(k3, c4);\n      h3 ^= k3;\n\n    case 8:\n      k2 ^= bytes[j + 7] << 24;\n\n    case 7:\n      k2 ^= bytes[j + 6] << 16;\n\n    case 6:\n      k2 ^= bytes[j + 5] << 8;\n\n    case 5:\n      k2 ^= bytes[j + 4];\n      k2 = _x86Multiply(k2, c2);\n      k2 = _x86Rotl(k2, 16);\n      k2 = _x86Multiply(k2, c3);\n      h2 ^= k2;\n\n    case 4:\n      k1 ^= bytes[j + 3] << 24;\n\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n\n  h1 ^= bytes.length;\n  h2 ^= bytes.length;\n  h3 ^= bytes.length;\n  h4 ^= bytes.length;\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n  h1 = _x86Fmix(h1);\n  h2 = _x86Fmix(h2);\n  h3 = _x86Fmix(h3);\n  h4 = _x86Fmix(h4);\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n  return (\"00000000\" + (h1 >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h2 >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h3 >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h4 >>> 0).toString(16)).slice(-8);\n}\n\nfunction x64Hash128(bytes, seed) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x64 flavor of MurmurHash3, as an unsigned hex.\n  //\n  seed = seed || 0;\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n  let h1 = [0, seed];\n  let h2 = [0, seed];\n  let k1 = [0, 0];\n  let k2 = [0, 0];\n  const c1 = [0x87c37b91, 0x114253d5];\n  const c2 = [0x4cf5ad43, 0x2745937f];\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = [bytes[i + 4] | bytes[i + 5] << 8 | bytes[i + 6] << 16 | bytes[i + 7] << 24, bytes[i] | bytes[i + 1] << 8 | bytes[i + 2] << 16 | bytes[i + 3] << 24];\n    k2 = [bytes[i + 12] | bytes[i + 13] << 8 | bytes[i + 14] << 16 | bytes[i + 15] << 24, bytes[i + 8] | bytes[i + 9] << 8 | bytes[i + 10] << 16 | bytes[i + 11] << 24];\n    k1 = _x64Multiply(k1, c1);\n    k1 = _x64Rotl(k1, 31);\n    k1 = _x64Multiply(k1, c2);\n    h1 = _x64Xor(h1, k1);\n    h1 = _x64Rotl(h1, 27);\n    h1 = _x64Add(h1, h2);\n    h1 = _x64Add(_x64Multiply(h1, [0, 5]), [0, 0x52dce729]);\n    k2 = _x64Multiply(k2, c2);\n    k2 = _x64Rotl(k2, 33);\n    k2 = _x64Multiply(k2, c1);\n    h2 = _x64Xor(h2, k2);\n    h2 = _x64Rotl(h2, 31);\n    h2 = _x64Add(h2, h1);\n    h2 = _x64Add(_x64Multiply(h2, [0, 5]), [0, 0x38495ab5]);\n    j = i + 16;\n  }\n\n  k1 = [0, 0];\n  k2 = [0, 0];\n\n  switch (remainder) {\n    case 15:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 14]], 48));\n\n    case 14:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 13]], 40));\n\n    case 13:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 12]], 32));\n\n    case 12:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 11]], 24));\n\n    case 11:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 10]], 16));\n\n    case 10:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 9]], 8));\n\n    case 9:\n      k2 = _x64Xor(k2, [0, bytes[j + 8]]);\n      k2 = _x64Multiply(k2, c2);\n      k2 = _x64Rotl(k2, 33);\n      k2 = _x64Multiply(k2, c1);\n      h2 = _x64Xor(h2, k2);\n\n    case 8:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 7]], 56));\n\n    case 7:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 6]], 48));\n\n    case 6:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 5]], 40));\n\n    case 5:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 4]], 32));\n\n    case 4:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 3]], 24));\n\n    case 3:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 2]], 16));\n\n    case 2:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 1]], 8));\n\n    case 1:\n      k1 = _x64Xor(k1, [0, bytes[j]]);\n      k1 = _x64Multiply(k1, c1);\n      k1 = _x64Rotl(k1, 31);\n      k1 = _x64Multiply(k1, c2);\n      h1 = _x64Xor(h1, k1);\n  }\n\n  h1 = _x64Xor(h1, [0, bytes.length]);\n  h2 = _x64Xor(h2, [0, bytes.length]);\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1);\n  h1 = _x64Fmix(h1);\n  h2 = _x64Fmix(h2);\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1); // Here we reverse h1 and h2 in Cosmos\n  // This is an implementation detail and not part of the public spec\n\n  const h1Buff = Buffer.from((\"00000000\" + (h1[0] >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h1[1] >>> 0).toString(16)).slice(-8), \"hex\");\n  const h1Reversed = reverse(h1Buff).toString(\"hex\");\n  const h2Buff = Buffer.from((\"00000000\" + (h2[0] >>> 0).toString(16)).slice(-8) + (\"00000000\" + (h2[1] >>> 0).toString(16)).slice(-8), \"hex\");\n  const h2Reversed = reverse(h2Buff).toString(\"hex\");\n  return h1Reversed + h2Reversed;\n}\n\nexport function reverse(buff) {\n  const buffer = Buffer.allocUnsafe(buff.length);\n\n  for (let i = 0, j = buff.length - 1; i <= j; ++i, --j) {\n    buffer[i] = buff[j];\n    buffer[j] = buff[i];\n  }\n\n  return buffer;\n}\nexport default {\n  version: \"3.0.0\",\n  x86: {\n    hash32: x86Hash32,\n    hash128: x86Hash128\n  },\n  x64: {\n    hash128: x64Hash128\n  },\n  inputValidation: true\n};","map":{"version":3,"sources":["../../../src/utils/hashing/murmurHash.ts"],"names":[],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA,SAAS,YAAT,CAAsB,CAAtB,EAAiC,CAAjC,EAA0C;AACxC;AACA;AACA;AACA;AAEA,SAAO,CAAC,CAAC,GAAG,MAAL,IAAe,CAAf,IAAoB,CAAE,CAAC,CAAC,KAAK,EAAP,IAAa,CAAd,GAAmB,MAApB,KAA+B,EAAnD,CAAP;AACD;;AAED,SAAS,QAAT,CAAkB,CAAlB,EAA6B,CAA7B,EAAsC;AACpC;AACA;AACA;AACA;AAEA,SAAQ,CAAC,IAAI,CAAN,GAAY,CAAC,KAAM,KAAK,CAA/B;AACD;;AAED,SAAS,QAAT,CAAkB,CAAlB,EAA2B;AACzB;AACA;AACA;AAEA,EAAA,CAAC,IAAI,CAAC,KAAK,EAAX;AACA,EAAA,CAAC,GAAG,YAAY,CAAC,CAAD,EAAI,UAAJ,CAAhB;AACA,EAAA,CAAC,IAAI,CAAC,KAAK,EAAX;AACA,EAAA,CAAC,GAAG,YAAY,CAAC,CAAD,EAAI,UAAJ,CAAhB;AACA,EAAA,CAAC,IAAI,CAAC,KAAK,EAAX;AAEA,SAAO,CAAP;AACD;;AAED,SAAS,OAAT,CAAiB,CAAjB,EAA8B,CAA9B,EAAyC;AACvC;AACA;AACA;AACA;AAEA,EAAA,CAAC,GAAG,CAAC,CAAC,CAAC,CAAD,CAAD,KAAS,EAAV,EAAc,CAAC,CAAC,CAAD,CAAD,GAAO,MAArB,EAA6B,CAAC,CAAC,CAAD,CAAD,KAAS,EAAtC,EAA0C,CAAC,CAAC,CAAD,CAAD,GAAO,MAAjD,CAAJ;AACA,EAAA,CAAC,GAAG,CAAC,CAAC,CAAC,CAAD,CAAD,KAAS,EAAV,EAAc,CAAC,CAAC,CAAD,CAAD,GAAO,MAArB,EAA6B,CAAC,CAAC,CAAD,CAAD,KAAS,EAAtC,EAA0C,CAAC,CAAC,CAAD,CAAD,GAAO,MAAjD,CAAJ;AACA,QAAM,CAAC,GAAG,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,CAAV,CAAV;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,SAAO,CAAE,CAAC,CAAC,CAAD,CAAD,IAAQ,EAAT,GAAe,CAAC,CAAC,CAAD,CAAjB,EAAuB,CAAC,CAAC,CAAD,CAAD,IAAQ,EAAT,GAAe,CAAC,CAAC,CAAD,CAAtC,CAAP;AACD;;AAED,SAAS,YAAT,CAAsB,CAAtB,EAAmC,CAAnC,EAA8C;AAC5C;AACA;AACA;AACA;AAEA,EAAA,CAAC,GAAG,CAAC,CAAC,CAAC,CAAD,CAAD,KAAS,EAAV,EAAc,CAAC,CAAC,CAAD,CAAD,GAAO,MAArB,EAA6B,CAAC,CAAC,CAAD,CAAD,KAAS,EAAtC,EAA0C,CAAC,CAAC,CAAD,CAAD,GAAO,MAAjD,CAAJ;AACA,EAAA,CAAC,GAAG,CAAC,CAAC,CAAC,CAAD,CAAD,KAAS,EAAV,EAAc,CAAC,CAAC,CAAD,CAAD,GAAO,MAArB,EAA6B,CAAC,CAAC,CAAD,CAAD,KAAS,EAAtC,EAA0C,CAAC,CAAC,CAAD,CAAD,GAAO,MAAjD,CAAJ;AACA,QAAM,CAAC,GAAG,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,CAAV,CAAV;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAhB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,KAAS,EAAjB;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAR,GAAc,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAtB,GAA4B,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAApC,GAA0C,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAA1D;AACA,EAAA,CAAC,CAAC,CAAD,CAAD,IAAQ,MAAR;AAEA,SAAO,CAAE,CAAC,CAAC,CAAD,CAAD,IAAQ,EAAT,GAAe,CAAC,CAAC,CAAD,CAAjB,EAAuB,CAAC,CAAC,CAAD,CAAD,IAAQ,EAAT,GAAe,CAAC,CAAC,CAAD,CAAtC,CAAP;AACD;;AAED,SAAS,QAAT,CAAkB,CAAlB,EAA+B,CAA/B,EAAwC;AACtC;AACA;AACA;AACA;AACA;AAEA,EAAA,CAAC,IAAI,EAAL;;AAEA,MAAI,CAAC,KAAK,EAAV,EAAc;AACZ,WAAO,CAAC,CAAC,CAAC,CAAD,CAAF,EAAO,CAAC,CAAC,CAAD,CAAR,CAAP;AACD,GAFD,MAEO,IAAI,CAAC,GAAG,EAAR,EAAY;AACjB,WAAO,CAAE,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAT,GAAe,CAAC,CAAC,CAAD,CAAD,KAAU,KAAK,CAA/B,EAAqC,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAT,GAAe,CAAC,CAAC,CAAD,CAAD,KAAU,KAAK,CAAlE,CAAP;AACD,GAFM,MAEA;AACL,IAAA,CAAC,IAAI,EAAL;AACA,WAAO,CAAE,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAT,GAAe,CAAC,CAAC,CAAD,CAAD,KAAU,KAAK,CAA/B,EAAqC,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAT,GAAe,CAAC,CAAC,CAAD,CAAD,KAAU,KAAK,CAAlE,CAAP;AACD;AACF;;AAED,SAAS,aAAT,CAAuB,CAAvB,EAAoC,CAApC,EAA6C;AAC3C;AACA;AACA;AACA;AACA;AAEA,EAAA,CAAC,IAAI,EAAL;;AAEA,MAAI,CAAC,KAAK,CAAV,EAAa;AACX,WAAO,CAAP;AACD,GAFD,MAEO,IAAI,CAAC,GAAG,EAAR,EAAY;AACjB,WAAO,CAAE,CAAC,CAAC,CAAD,CAAD,IAAQ,CAAT,GAAe,CAAC,CAAC,CAAD,CAAD,KAAU,KAAK,CAA/B,EAAoC,CAAC,CAAC,CAAD,CAAD,IAAQ,CAA5C,CAAP;AACD,GAFM,MAEA;AACL,WAAO,CAAC,CAAC,CAAC,CAAD,CAAD,IAAS,CAAC,GAAG,EAAd,EAAmB,CAAnB,CAAP;AACD;AACF;;AAED,SAAS,OAAT,CAAiB,CAAjB,EAA8B,CAA9B,EAAyC;AACvC;AACA;AACA;AACA;AAEA,SAAO,CAAC,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAT,EAAc,CAAC,CAAC,CAAD,CAAD,GAAO,CAAC,CAAC,CAAD,CAAtB,CAAP;AACD;;AAED,SAAS,QAAT,CAAkB,CAAlB,EAA6B;AAC3B;AACA;AACA;AACA;AACA;AAEA,EAAA,CAAC,GAAG,OAAO,CAAC,CAAD,EAAI,CAAC,CAAD,EAAI,CAAC,CAAC,CAAD,CAAD,KAAS,CAAb,CAAJ,CAAX;AACA,EAAA,CAAC,GAAG,YAAY,CAAC,CAAD,EAAI,CAAC,UAAD,EAAa,UAAb,CAAJ,CAAhB;AACA,EAAA,CAAC,GAAG,OAAO,CAAC,CAAD,EAAI,CAAC,CAAD,EAAI,CAAC,CAAC,CAAD,CAAD,KAAS,CAAb,CAAJ,CAAX;AACA,EAAA,CAAC,GAAG,YAAY,CAAC,CAAD,EAAI,CAAC,UAAD,EAAa,UAAb,CAAJ,CAAhB;AACA,EAAA,CAAC,GAAG,OAAO,CAAC,CAAD,EAAI,CAAC,CAAD,EAAI,CAAC,CAAC,CAAD,CAAD,KAAS,CAAb,CAAJ,CAAX;AAEA,SAAO,CAAP;AACD,C,CAED;AACA;;;AAEA,SAAS,SAAT,CAAmB,KAAnB,EAAkC,IAAlC,EAA+C;AAC7C;AACA;AACA;AACA;AACA,EAAA,IAAI,GAAG,IAAI,IAAI,CAAf;AAEA,QAAM,SAAS,GAAG,KAAK,CAAC,MAAN,GAAe,CAAjC;AACA,QAAM,MAAM,GAAG,KAAK,CAAC,MAAN,GAAe,SAA9B;AAEA,MAAI,EAAE,GAAG,IAAT;AAEA,MAAI,EAAE,GAAG,CAAT;AAEA,QAAM,EAAE,GAAG,UAAX;AACA,QAAM,EAAE,GAAG,UAAX;AACA,MAAI,CAAC,GAAG,CAAR;;AAEA,OAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,MAApB,EAA4B,CAAC,GAAG,CAAC,GAAG,CAApC,EAAuC;AACrC,IAAA,EAAE,GAAG,KAAK,CAAC,CAAD,CAAL,GAAY,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAA5B,GAAkC,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAlD,GAAyD,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAA9E;AAEA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AAEA,IAAA,EAAE,IAAI,EAAN;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,CAAL,CAAZ,GAAsB,UAA3B;AACA,IAAA,CAAC,GAAG,CAAC,GAAG,CAAR;AACD;;AAED,EAAA,EAAE,GAAG,CAAL;;AAEA,UAAQ,SAAR;AACE,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAD,CAAX;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,IAAI,EAAN;AAZJ;;AAeA,EAAA,EAAE,IAAI,KAAK,CAAC,MAAZ;AACA,EAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,CAAb;AAEA,SAAO,EAAE,KAAK,CAAd;AACD;;AAED,SAAS,UAAT,CAAoB,KAApB,EAAmC,IAAnC,EAAgD;AAC9C;AACA;AACA;AACA;AAEA,EAAA,IAAI,GAAG,IAAI,IAAI,CAAf;AACA,QAAM,SAAS,GAAG,KAAK,CAAC,MAAN,GAAe,EAAjC;AACA,QAAM,MAAM,GAAG,KAAK,CAAC,MAAN,GAAe,SAA9B;AAEA,MAAI,EAAE,GAAG,IAAT;AACA,MAAI,EAAE,GAAG,IAAT;AACA,MAAI,EAAE,GAAG,IAAT;AACA,MAAI,EAAE,GAAG,IAAT;AAEA,MAAI,EAAE,GAAG,CAAT;AACA,MAAI,EAAE,GAAG,CAAT;AACA,MAAI,EAAE,GAAG,CAAT;AACA,MAAI,EAAE,GAAG,CAAT;AAEA,QAAM,EAAE,GAAG,UAAX;AACA,QAAM,EAAE,GAAG,UAAX;AACA,QAAM,EAAE,GAAG,UAAX;AACA,QAAM,EAAE,GAAG,UAAX;AACA,MAAI,CAAC,GAAG,CAAR;;AAEA,OAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,MAApB,EAA4B,CAAC,GAAG,CAAC,GAAG,EAApC,EAAwC;AACtC,IAAA,EAAE,GAAG,KAAK,CAAC,CAAD,CAAL,GAAY,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAA5B,GAAkC,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAlD,GAAyD,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAA9E;AACA,IAAA,EAAE,GAAG,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,GAAgB,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAhC,GAAsC,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAtD,GAA6D,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAlF;AACA,IAAA,EAAE,GAAG,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,GAAgB,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAhC,GAAsC,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAvD,GAA8D,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAApF;AACA,IAAA,EAAE,GAAG,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,GAAiB,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,CAAlC,GAAwC,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAzD,GAAgE,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAtF;AAEA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,IAAI,EAAN;AAEA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,IAAI,EAAN;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,CAAL,CAAZ,GAAsB,UAA3B;AAEA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,IAAI,EAAN;AAEA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,IAAI,EAAN;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,CAAL,CAAZ,GAAsB,UAA3B;AAEA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,IAAI,EAAN;AAEA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,IAAI,EAAN;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,CAAL,CAAZ,GAAsB,UAA3B;AAEA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,IAAI,EAAN;AAEA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,IAAI,EAAN;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,CAAL,CAAZ,GAAsB,UAA3B;AACA,IAAA,CAAC,GAAG,CAAC,GAAG,EAAR;AACD;;AAED,EAAA,EAAE,GAAG,CAAL;AACA,EAAA,EAAE,GAAG,CAAL;AACA,EAAA,EAAE,GAAG,CAAL;AACA,EAAA,EAAE,GAAG,CAAL;;AAEA,UAAQ,SAAR;AACE,SAAK,EAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAvB;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,CAAvB;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAX;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,IAAI,EAAN;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAvB;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAvB;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAX;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,IAAI,EAAN;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAX;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,IAAI,EAAN;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAtB;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,IAAI,KAAK,CAAC,CAAD,CAAX;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,IAAI,EAAN;AA5DJ;;AA+DA,EAAA,EAAE,IAAI,KAAK,CAAC,MAAZ;AACA,EAAA,EAAE,IAAI,KAAK,CAAC,MAAZ;AACA,EAAA,EAAE,IAAI,KAAK,CAAC,MAAZ;AACA,EAAA,EAAE,IAAI,KAAK,CAAC,MAAZ;AAEA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AAEA,EAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,CAAb;AACA,EAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,CAAb;AACA,EAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,CAAb;AACA,EAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,CAAb;AAEA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AACA,EAAA,EAAE,IAAI,EAAN;AAEA,SACE,CAAC,aAAa,CAAC,EAAE,KAAK,CAAR,EAAW,QAAX,CAAoB,EAApB,CAAd,EAAuC,KAAvC,CAA6C,CAAC,CAA9C,IACA,CAAC,aAAa,CAAC,EAAE,KAAK,CAAR,EAAW,QAAX,CAAoB,EAApB,CAAd,EAAuC,KAAvC,CAA6C,CAAC,CAA9C,CADA,GAEA,CAAC,aAAa,CAAC,EAAE,KAAK,CAAR,EAAW,QAAX,CAAoB,EAApB,CAAd,EAAuC,KAAvC,CAA6C,CAAC,CAA9C,CAFA,GAGA,CAAC,aAAa,CAAC,EAAE,KAAK,CAAR,EAAW,QAAX,CAAoB,EAApB,CAAd,EAAuC,KAAvC,CAA6C,CAAC,CAA9C,CAJF;AAMD;;AAED,SAAS,UAAT,CAAoB,KAApB,EAAmC,IAAnC,EAAgD;AAC9C;AACA;AACA;AACA;AACA,EAAA,IAAI,GAAG,IAAI,IAAI,CAAf;AAEA,QAAM,SAAS,GAAG,KAAK,CAAC,MAAN,GAAe,EAAjC;AACA,QAAM,MAAM,GAAG,KAAK,CAAC,MAAN,GAAe,SAA9B;AAEA,MAAI,EAAE,GAAG,CAAC,CAAD,EAAI,IAAJ,CAAT;AACA,MAAI,EAAE,GAAG,CAAC,CAAD,EAAI,IAAJ,CAAT;AAEA,MAAI,EAAE,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAT;AACA,MAAI,EAAE,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAT;AAEA,QAAM,EAAE,GAAG,CAAC,UAAD,EAAa,UAAb,CAAX;AACA,QAAM,EAAE,GAAG,CAAC,UAAD,EAAa,UAAb,CAAX;AACA,MAAI,CAAC,GAAG,CAAR;;AAEA,OAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,MAApB,EAA4B,CAAC,GAAG,CAAC,GAAG,EAApC,EAAwC;AACtC,IAAA,EAAE,GAAG,CACH,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,GAAgB,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAhC,GAAsC,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAtD,GAA6D,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAD1E,EAEH,KAAK,CAAC,CAAD,CAAL,GAAY,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAA5B,GAAkC,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAAlD,GAAyD,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,EAFtE,CAAL;AAIA,IAAA,EAAE,GAAG,CACH,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,GAAiB,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,CAAlC,GAAwC,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAzD,GAAgE,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAD9E,EAEH,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,GAAgB,KAAK,CAAC,CAAC,GAAG,CAAL,CAAL,IAAgB,CAAhC,GAAsC,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAAvD,GAA8D,KAAK,CAAC,CAAC,GAAG,EAAL,CAAL,IAAiB,EAF5E,CAAL;AAKA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AAEA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AACA,IAAA,EAAE,GAAG,OAAO,CAAC,YAAY,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,CAAJ,CAAL,CAAb,EAA2B,CAAC,CAAD,EAAI,UAAJ,CAA3B,CAAZ;AAEA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,IAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AAEA,IAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,IAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AACA,IAAA,EAAE,GAAG,OAAO,CAAC,YAAY,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,CAAJ,CAAL,CAAb,EAA2B,CAAC,CAAD,EAAI,UAAJ,CAA3B,CAAZ;AACA,IAAA,CAAC,GAAG,CAAC,GAAG,EAAR;AACD;;AAED,EAAA,EAAE,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAL;AACA,EAAA,EAAE,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAL;;AAEA,UAAQ,SAAR;AACE,SAAK,EAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAT,CAAD,EAAqB,EAArB,CAAlB,CAAZ;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAT,CAAD,EAAqB,EAArB,CAAlB,CAAZ;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAT,CAAD,EAAqB,EAArB,CAAlB,CAAZ;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAT,CAAD,EAAqB,EAArB,CAAlB,CAAZ;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,EAAL,CAAT,CAAD,EAAqB,EAArB,CAAlB,CAAZ;;AAEF,SAAK,EAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,CAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAL,CAAZ;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,EAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,EAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,EAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,EAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,EAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,EAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,aAAa,CAAC,CAAC,CAAD,EAAI,KAAK,CAAC,CAAC,GAAG,CAAL,CAAT,CAAD,EAAoB,CAApB,CAAlB,CAAZ;;AAEF,SAAK,CAAL;AACE,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,KAAK,CAAC,CAAD,CAAT,CAAL,CAAZ;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,EAAK,EAAL,CAAb;AACA,MAAA,EAAE,GAAG,YAAY,CAAC,EAAD,EAAK,EAAL,CAAjB;AACA,MAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AApDJ;;AAuDA,EAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,KAAK,CAAC,MAAV,CAAL,CAAZ;AACA,EAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,KAAK,CAAC,MAAV,CAAL,CAAZ;AAEA,EAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AACA,EAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AAEA,EAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,CAAb;AACA,EAAA,EAAE,GAAG,QAAQ,CAAC,EAAD,CAAb;AAEA,EAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ;AACA,EAAA,EAAE,GAAG,OAAO,CAAC,EAAD,EAAK,EAAL,CAAZ,CAtH8C,CAwH9C;AACA;;AACA,QAAM,MAAM,GAAG,MAAM,CAAC,IAAP,CACb,CAAC,aAAa,CAAC,EAAE,CAAC,CAAD,CAAF,KAAU,CAAX,EAAc,QAAd,CAAuB,EAAvB,CAAd,EAA0C,KAA1C,CAAgD,CAAC,CAAjD,IACE,CAAC,aAAa,CAAC,EAAE,CAAC,CAAD,CAAF,KAAU,CAAX,EAAc,QAAd,CAAuB,EAAvB,CAAd,EAA0C,KAA1C,CAAgD,CAAC,CAAjD,CAFW,EAGb,KAHa,CAAf;AAKA,QAAM,UAAU,GAAG,OAAO,CAAC,MAAD,CAAP,CAAgB,QAAhB,CAAyB,KAAzB,CAAnB;AACA,QAAM,MAAM,GAAG,MAAM,CAAC,IAAP,CACb,CAAC,aAAa,CAAC,EAAE,CAAC,CAAD,CAAF,KAAU,CAAX,EAAc,QAAd,CAAuB,EAAvB,CAAd,EAA0C,KAA1C,CAAgD,CAAC,CAAjD,IACE,CAAC,aAAa,CAAC,EAAE,CAAC,CAAD,CAAF,KAAU,CAAX,EAAc,QAAd,CAAuB,EAAvB,CAAd,EAA0C,KAA1C,CAAgD,CAAC,CAAjD,CAFW,EAGb,KAHa,CAAf;AAKA,QAAM,UAAU,GAAG,OAAO,CAAC,MAAD,CAAP,CAAgB,QAAhB,CAAyB,KAAzB,CAAnB;AACA,SAAO,UAAU,GAAG,UAApB;AACD;;AAED,OAAM,SAAU,OAAV,CAAkB,IAAlB,EAA8B;AAClC,QAAM,MAAM,GAAG,MAAM,CAAC,WAAP,CAAmB,IAAI,CAAC,MAAxB,CAAf;;AAEA,OAAK,IAAI,CAAC,GAAG,CAAR,EAAW,CAAC,GAAG,IAAI,CAAC,MAAL,GAAc,CAAlC,EAAqC,CAAC,IAAI,CAA1C,EAA6C,EAAE,CAAF,EAAK,EAAE,CAApD,EAAuD;AACrD,IAAA,MAAM,CAAC,CAAD,CAAN,GAAY,IAAI,CAAC,CAAD,CAAhB;AACA,IAAA,MAAM,CAAC,CAAD,CAAN,GAAY,IAAI,CAAC,CAAD,CAAhB;AACD;;AACD,SAAO,MAAP;AACD;AAED,eAAe;AACb,EAAA,OAAO,EAAE,OADI;AAEb,EAAA,GAAG,EAAE;AACH,IAAA,MAAM,EAAE,SADL;AAEH,IAAA,OAAO,EAAE;AAFN,GAFQ;AAMb,EAAA,GAAG,EAAE;AACH,IAAA,OAAO,EAAE;AADN,GANQ;AASb,EAAA,eAAe,EAAE;AATJ,CAAf","sourcesContent":["// +----------------------------------------------------------------------+\n// | murmurHash3js.js v3.0.1 // https://github.com/pid/murmurHash3js\n// | A javascript implementation of MurmurHash3's x86 hashing algorithms. |\n// |----------------------------------------------------------------------|\n// | Copyright (c) 2012-2015 Karan Lyons                                       |\n// | https://github.com/karanlyons/murmurHash3.js/blob/c1778f75792abef7bdd74bc85d2d4e1a3d25cfe9/murmurHash3.js |\n// | Freely distributable under the MIT license.                          |\n// +----------------------------------------------------------------------+\n\n// PRIVATE FUNCTIONS\n// -----------------\n\nfunction _x86Multiply(m: number, n: number) {\n  //\n  // Given two 32bit ints, returns the two multiplied together as a\n  // 32bit int.\n  //\n\n  return (m & 0xffff) * n + ((((m >>> 16) * n) & 0xffff) << 16);\n}\n\nfunction _x86Rotl(m: number, n: number) {\n  //\n  // Given a 32bit int and an int representing a number of bit positions,\n  // returns the 32bit int rotated left by that number of positions.\n  //\n\n  return (m << n) | (m >>> (32 - n));\n}\n\nfunction _x86Fmix(h: number) {\n  //\n  // Given a block, returns murmurHash3's final x86 mix of that block.\n  //\n\n  h ^= h >>> 16;\n  h = _x86Multiply(h, 0x85ebca6b);\n  h ^= h >>> 13;\n  h = _x86Multiply(h, 0xc2b2ae35);\n  h ^= h >>> 16;\n\n  return h;\n}\n\nfunction _x64Add(m: number[], n: number[]) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // added together as a 64bit int (as an array of two 32bit ints).\n  //\n\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n\n  o[3] += m[3] + n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n\n  o[2] += m[2] + n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n\n  o[1] += m[1] + n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[0] += m[0] + n[0];\n  o[0] &= 0xffff;\n\n  return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]];\n}\n\nfunction _x64Multiply(m: number[], n: number[]) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // multiplied together as a 64bit int (as an array of two 32bit ints).\n  //\n\n  m = [m[0] >>> 16, m[0] & 0xffff, m[1] >>> 16, m[1] & 0xffff];\n  n = [n[0] >>> 16, n[0] & 0xffff, n[1] >>> 16, n[1] & 0xffff];\n  const o = [0, 0, 0, 0];\n\n  o[3] += m[3] * n[3];\n  o[2] += o[3] >>> 16;\n  o[3] &= 0xffff;\n\n  o[2] += m[2] * n[3];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n\n  o[2] += m[3] * n[2];\n  o[1] += o[2] >>> 16;\n  o[2] &= 0xffff;\n\n  o[1] += m[1] * n[3];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[1] += m[2] * n[2];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[1] += m[3] * n[1];\n  o[0] += o[1] >>> 16;\n  o[1] &= 0xffff;\n\n  o[0] += m[0] * n[3] + m[1] * n[2] + m[2] * n[1] + m[3] * n[0];\n  o[0] &= 0xffff;\n\n  return [(o[0] << 16) | o[1], (o[2] << 16) | o[3]];\n}\n\nfunction _x64Rotl(m: number[], n: number) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) rotated left by that number of positions.\n  //\n\n  n %= 64;\n\n  if (n === 32) {\n    return [m[1], m[0]];\n  } else if (n < 32) {\n    return [(m[0] << n) | (m[1] >>> (32 - n)), (m[1] << n) | (m[0] >>> (32 - n))];\n  } else {\n    n -= 32;\n    return [(m[1] << n) | (m[0] >>> (32 - n)), (m[0] << n) | (m[1] >>> (32 - n))];\n  }\n}\n\nfunction _x64LeftShift(m: number[], n: number) {\n  //\n  // Given a 64bit int (as an array of two 32bit ints) and an int\n  // representing a number of bit positions, returns the 64bit int (as an\n  // array of two 32bit ints) shifted left by that number of positions.\n  //\n\n  n %= 64;\n\n  if (n === 0) {\n    return m;\n  } else if (n < 32) {\n    return [(m[0] << n) | (m[1] >>> (32 - n)), m[1] << n];\n  } else {\n    return [m[1] << (n - 32), 0];\n  }\n}\n\nfunction _x64Xor(m: number[], n: number[]) {\n  //\n  // Given two 64bit ints (as an array of two 32bit ints) returns the two\n  // xored together as a 64bit int (as an array of two 32bit ints).\n  //\n\n  return [m[0] ^ n[0], m[1] ^ n[1]];\n}\n\nfunction _x64Fmix(h: number[]) {\n  //\n  // Given a block, returns murmurHash3's final x64 mix of that block.\n  // (`[0, h[0] >>> 1]` is a 33 bit unsigned right shift. This is the\n  // only place where we need to right shift 64bit ints.)\n  //\n\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xff51afd7, 0xed558ccd]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n  h = _x64Multiply(h, [0xc4ceb9fe, 0x1a85ec53]);\n  h = _x64Xor(h, [0, h[0] >>> 1]);\n\n  return h;\n}\n\n// PUBLIC FUNCTIONS\n// ----------------\n\nfunction x86Hash32(bytes: Buffer, seed?: number) {\n  //\n  // Given a string and an optional seed as an int, returns a 32 bit hash\n  // using the x86 flavor of MurmurHash3, as an unsigned int.\n  //\n  seed = seed || 0;\n\n  const remainder = bytes.length % 4;\n  const blocks = bytes.length - remainder;\n\n  let h1 = seed;\n\n  let k1 = 0;\n\n  const c1 = 0xcc9e2d51;\n  const c2 = 0x1b873593;\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 4) {\n    k1 = bytes[i] | (bytes[i + 1] << 8) | (bytes[i + 2] << 16) | (bytes[i + 3] << 24);\n\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n\n    h1 ^= k1;\n    h1 = _x86Rotl(h1, 13);\n    h1 = _x86Multiply(h1, 5) + 0xe6546b64;\n    j = i + 4;\n  }\n\n  k1 = 0;\n\n  switch (remainder) {\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n\n  h1 ^= bytes.length;\n  h1 = _x86Fmix(h1);\n\n  return h1 >>> 0;\n}\n\nfunction x86Hash128(bytes: Buffer, seed?: number) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x86 flavor of MurmurHash3, as an unsigned hex.\n  //\n\n  seed = seed || 0;\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n\n  let h1 = seed;\n  let h2 = seed;\n  let h3 = seed;\n  let h4 = seed;\n\n  let k1 = 0;\n  let k2 = 0;\n  let k3 = 0;\n  let k4 = 0;\n\n  const c1 = 0x239b961b;\n  const c2 = 0xab0e9789;\n  const c3 = 0x38b34ae5;\n  const c4 = 0xa1e38b93;\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = bytes[i] | (bytes[i + 1] << 8) | (bytes[i + 2] << 16) | (bytes[i + 3] << 24);\n    k2 = bytes[i + 4] | (bytes[i + 5] << 8) | (bytes[i + 6] << 16) | (bytes[i + 7] << 24);\n    k3 = bytes[i + 8] | (bytes[i + 9] << 8) | (bytes[i + 10] << 16) | (bytes[i + 11] << 24);\n    k4 = bytes[i + 12] | (bytes[i + 13] << 8) | (bytes[i + 14] << 16) | (bytes[i + 15] << 24);\n\n    k1 = _x86Multiply(k1, c1);\n    k1 = _x86Rotl(k1, 15);\n    k1 = _x86Multiply(k1, c2);\n    h1 ^= k1;\n\n    h1 = _x86Rotl(h1, 19);\n    h1 += h2;\n    h1 = _x86Multiply(h1, 5) + 0x561ccd1b;\n\n    k2 = _x86Multiply(k2, c2);\n    k2 = _x86Rotl(k2, 16);\n    k2 = _x86Multiply(k2, c3);\n    h2 ^= k2;\n\n    h2 = _x86Rotl(h2, 17);\n    h2 += h3;\n    h2 = _x86Multiply(h2, 5) + 0x0bcaa747;\n\n    k3 = _x86Multiply(k3, c3);\n    k3 = _x86Rotl(k3, 17);\n    k3 = _x86Multiply(k3, c4);\n    h3 ^= k3;\n\n    h3 = _x86Rotl(h3, 15);\n    h3 += h4;\n    h3 = _x86Multiply(h3, 5) + 0x96cd1c35;\n\n    k4 = _x86Multiply(k4, c4);\n    k4 = _x86Rotl(k4, 18);\n    k4 = _x86Multiply(k4, c1);\n    h4 ^= k4;\n\n    h4 = _x86Rotl(h4, 13);\n    h4 += h1;\n    h4 = _x86Multiply(h4, 5) + 0x32ac3b17;\n    j = i + 16;\n  }\n\n  k1 = 0;\n  k2 = 0;\n  k3 = 0;\n  k4 = 0;\n\n  switch (remainder) {\n    case 15:\n      k4 ^= bytes[j + 14] << 16;\n\n    case 14:\n      k4 ^= bytes[j + 13] << 8;\n\n    case 13:\n      k4 ^= bytes[j + 12];\n      k4 = _x86Multiply(k4, c4);\n      k4 = _x86Rotl(k4, 18);\n      k4 = _x86Multiply(k4, c1);\n      h4 ^= k4;\n\n    case 12:\n      k3 ^= bytes[j + 11] << 24;\n\n    case 11:\n      k3 ^= bytes[j + 10] << 16;\n\n    case 10:\n      k3 ^= bytes[j + 9] << 8;\n\n    case 9:\n      k3 ^= bytes[j + 8];\n      k3 = _x86Multiply(k3, c3);\n      k3 = _x86Rotl(k3, 17);\n      k3 = _x86Multiply(k3, c4);\n      h3 ^= k3;\n\n    case 8:\n      k2 ^= bytes[j + 7] << 24;\n\n    case 7:\n      k2 ^= bytes[j + 6] << 16;\n\n    case 6:\n      k2 ^= bytes[j + 5] << 8;\n\n    case 5:\n      k2 ^= bytes[j + 4];\n      k2 = _x86Multiply(k2, c2);\n      k2 = _x86Rotl(k2, 16);\n      k2 = _x86Multiply(k2, c3);\n      h2 ^= k2;\n\n    case 4:\n      k1 ^= bytes[j + 3] << 24;\n\n    case 3:\n      k1 ^= bytes[j + 2] << 16;\n\n    case 2:\n      k1 ^= bytes[j + 1] << 8;\n\n    case 1:\n      k1 ^= bytes[j];\n      k1 = _x86Multiply(k1, c1);\n      k1 = _x86Rotl(k1, 15);\n      k1 = _x86Multiply(k1, c2);\n      h1 ^= k1;\n  }\n\n  h1 ^= bytes.length;\n  h2 ^= bytes.length;\n  h3 ^= bytes.length;\n  h4 ^= bytes.length;\n\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n\n  h1 = _x86Fmix(h1);\n  h2 = _x86Fmix(h2);\n  h3 = _x86Fmix(h3);\n  h4 = _x86Fmix(h4);\n\n  h1 += h2;\n  h1 += h3;\n  h1 += h4;\n  h2 += h1;\n  h3 += h1;\n  h4 += h1;\n\n  return (\n    (\"00000000\" + (h1 >>> 0).toString(16)).slice(-8) +\n    (\"00000000\" + (h2 >>> 0).toString(16)).slice(-8) +\n    (\"00000000\" + (h3 >>> 0).toString(16)).slice(-8) +\n    (\"00000000\" + (h4 >>> 0).toString(16)).slice(-8)\n  );\n}\n\nfunction x64Hash128(bytes: Buffer, seed?: number) {\n  //\n  // Given a string and an optional seed as an int, returns a 128 bit\n  // hash using the x64 flavor of MurmurHash3, as an unsigned hex.\n  //\n  seed = seed || 0;\n\n  const remainder = bytes.length % 16;\n  const blocks = bytes.length - remainder;\n\n  let h1 = [0, seed];\n  let h2 = [0, seed];\n\n  let k1 = [0, 0];\n  let k2 = [0, 0];\n\n  const c1 = [0x87c37b91, 0x114253d5];\n  const c2 = [0x4cf5ad43, 0x2745937f];\n  let j = 0;\n\n  for (let i = 0; i < blocks; i = i + 16) {\n    k1 = [\n      bytes[i + 4] | (bytes[i + 5] << 8) | (bytes[i + 6] << 16) | (bytes[i + 7] << 24),\n      bytes[i] | (bytes[i + 1] << 8) | (bytes[i + 2] << 16) | (bytes[i + 3] << 24)\n    ];\n    k2 = [\n      bytes[i + 12] | (bytes[i + 13] << 8) | (bytes[i + 14] << 16) | (bytes[i + 15] << 24),\n      bytes[i + 8] | (bytes[i + 9] << 8) | (bytes[i + 10] << 16) | (bytes[i + 11] << 24)\n    ];\n\n    k1 = _x64Multiply(k1, c1);\n    k1 = _x64Rotl(k1, 31);\n    k1 = _x64Multiply(k1, c2);\n    h1 = _x64Xor(h1, k1);\n\n    h1 = _x64Rotl(h1, 27);\n    h1 = _x64Add(h1, h2);\n    h1 = _x64Add(_x64Multiply(h1, [0, 5]), [0, 0x52dce729]);\n\n    k2 = _x64Multiply(k2, c2);\n    k2 = _x64Rotl(k2, 33);\n    k2 = _x64Multiply(k2, c1);\n    h2 = _x64Xor(h2, k2);\n\n    h2 = _x64Rotl(h2, 31);\n    h2 = _x64Add(h2, h1);\n    h2 = _x64Add(_x64Multiply(h2, [0, 5]), [0, 0x38495ab5]);\n    j = i + 16;\n  }\n\n  k1 = [0, 0];\n  k2 = [0, 0];\n\n  switch (remainder) {\n    case 15:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 14]], 48));\n\n    case 14:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 13]], 40));\n\n    case 13:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 12]], 32));\n\n    case 12:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 11]], 24));\n\n    case 11:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 10]], 16));\n\n    case 10:\n      k2 = _x64Xor(k2, _x64LeftShift([0, bytes[j + 9]], 8));\n\n    case 9:\n      k2 = _x64Xor(k2, [0, bytes[j + 8]]);\n      k2 = _x64Multiply(k2, c2);\n      k2 = _x64Rotl(k2, 33);\n      k2 = _x64Multiply(k2, c1);\n      h2 = _x64Xor(h2, k2);\n\n    case 8:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 7]], 56));\n\n    case 7:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 6]], 48));\n\n    case 6:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 5]], 40));\n\n    case 5:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 4]], 32));\n\n    case 4:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 3]], 24));\n\n    case 3:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 2]], 16));\n\n    case 2:\n      k1 = _x64Xor(k1, _x64LeftShift([0, bytes[j + 1]], 8));\n\n    case 1:\n      k1 = _x64Xor(k1, [0, bytes[j]]);\n      k1 = _x64Multiply(k1, c1);\n      k1 = _x64Rotl(k1, 31);\n      k1 = _x64Multiply(k1, c2);\n      h1 = _x64Xor(h1, k1);\n  }\n\n  h1 = _x64Xor(h1, [0, bytes.length]);\n  h2 = _x64Xor(h2, [0, bytes.length]);\n\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1);\n\n  h1 = _x64Fmix(h1);\n  h2 = _x64Fmix(h2);\n\n  h1 = _x64Add(h1, h2);\n  h2 = _x64Add(h2, h1);\n\n  // Here we reverse h1 and h2 in Cosmos\n  // This is an implementation detail and not part of the public spec\n  const h1Buff = Buffer.from(\n    (\"00000000\" + (h1[0] >>> 0).toString(16)).slice(-8) +\n      (\"00000000\" + (h1[1] >>> 0).toString(16)).slice(-8),\n    \"hex\"\n  );\n  const h1Reversed = reverse(h1Buff).toString(\"hex\");\n  const h2Buff = Buffer.from(\n    (\"00000000\" + (h2[0] >>> 0).toString(16)).slice(-8) +\n      (\"00000000\" + (h2[1] >>> 0).toString(16)).slice(-8),\n    \"hex\"\n  );\n  const h2Reversed = reverse(h2Buff).toString(\"hex\");\n  return h1Reversed + h2Reversed;\n}\n\nexport function reverse(buff: Buffer) {\n  const buffer = Buffer.allocUnsafe(buff.length);\n\n  for (let i = 0, j = buff.length - 1; i <= j; ++i, --j) {\n    buffer[i] = buff[j];\n    buffer[j] = buff[i];\n  }\n  return buffer;\n}\n\nexport default {\n  version: \"3.0.0\",\n  x86: {\n    hash32: x86Hash32,\n    hash128: x86Hash128\n  },\n  x64: {\n    hash128: x64Hash128\n  },\n  inputValidation: true\n};\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}